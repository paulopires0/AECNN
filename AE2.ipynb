{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import time \n",
    "\n",
    "from scipy.integrate import simpson\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"a2f_data.pk.bz2\")[:250] #Objetivo do [:250] é remover os pontos iguais a zero\n",
    "w = df.w\n",
    "df = df.drop(\"w\", axis=1).T\n",
    "df=df.drop([\"agm002693510\"]) #Está repetido com valores de NaN\n",
    "\n",
    "Ry2cm = 219474.63/2\n",
    "K_BOLTZMANN_RY = (1.380649E-23/(4.3597447222071E-18/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epc(a2f, w):\n",
    "  K_BOLTZMANN_RY = (1.380649E-23/(4.3597447222071E-18/2))\n",
    "  la = 2*simpson(a2f/w, w)\n",
    "  sim = simpson(a2f/w*np.log(w), w)\n",
    "  wlog = np.exp(2/la*sim)/K_BOLTZMANN_RY\n",
    "  sim = max(0.0, simpson(2*a2f*w, w))\n",
    "\n",
    "  w2 = np.sqrt(sim/la)/K_BOLTZMANN_RY\n",
    "  return wlog, w2, la\n",
    "\n",
    "def get_Tc(la, wlog, mu):\n",
    "  if (la > 1.5*mu) and (wlog > 0):\n",
    "    ad = wlog/1.2*np.exp(-(1.04*(1+la))/(la-mu*(1+0.62*la)))\n",
    "  else:\n",
    "    ad = 0.0\n",
    "  return ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.X_train = torch.tensor(data.values, dtype=torch.float32)[:,None]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5150 643 644\n",
      "0.800062140748796 0.09989125368960695 0.10004660556159702\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set, validation_set= np.split(df.sample(frac=1, random_state=52),[int(.1*len(df)),int(.9*len(df))])#[int(.8*len(df)),int(.9*len(df))])   random.randint(1, 100)[644,size_train+644]\n",
    "\n",
    "print(len(train_set),len(test_set),len(validation_set))\n",
    "print(len(train_set)/df.shape[0],len(test_set)/df.shape[0],len(validation_set)/df.shape[0])\n",
    "\n",
    "batch_size = train_set.shape[0]\n",
    "\n",
    "train_data= DataLoader(FeatureDataset(train_set), batch_size=128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpScale(nn.Module):\n",
    "    def __init__(self, size, scale, mode):\n",
    "        super(UpScale, self).__init__()\n",
    "        self.interpolate = nn.functional.interpolate\n",
    "        self.scale=scale\n",
    "        self.mode=mode\n",
    "        self.size=size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.size==0:\n",
    "            x = self.interpolate(x, scale_factor=self.scale, mode=self.mode, align_corners=False)\n",
    "            return x\n",
    "        elif self.scale==0:\n",
    "            x = self.interpolate(x, size=self.size, mode=self.mode, align_corners=False)\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, initial_size, min_size):\n",
    "        self.initial_size= initial_size\n",
    "        self.min_size= min_size\n",
    "\n",
    "        super(Autoencoder,self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(                                             #Channels*Dim\n",
    "            nn.Conv1d(1,8, kernel_size=(16,),stride=(2,),padding=(0,),bias=True), #1*250 -> 8*118\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.SiLU(), \n",
    "\n",
    "\n",
    "            nn.Conv1d(8, 16, kernel_size=(16,),stride=(1,),padding=(0,),bias=True), #8*118 -> 16*103\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(), \n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=(2,)), #16*103 -> 16*51\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=(16,),stride=(1,),padding=(0,),bias=True), #16*51 -> 32*36\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=(16,),stride=(2,),padding=(0,),bias=True), #32*36 -> 64*11\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Conv1d(64, 16, kernel_size=(8,),stride=(1,),padding=(0,),bias=True), #64*11 -> 16*4\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(64,32,bias=True),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(32,16,bias=True),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(16,8,bias=True),\n",
    "            nn.Tanh(), \n",
    "            )\n",
    "          \n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Linear(8,16,bias=True),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "\n",
    "            nn.Linear(16,32,bias=True),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(32,64,bias=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Unflatten(1,(16,4)),\n",
    "            \n",
    "            nn.ConvTranspose1d(16, 16, kernel_size=(8,),stride=(1,),padding=(0,),output_padding=(0,),bias=True), #16*4 -> 16*11\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.ConvTranspose1d(16, 64, kernel_size=(16,),stride=(2,),padding=(0,),output_padding=(0,),bias=True), #16*11 -> 64*36\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "\n",
    "\n",
    "            \n",
    "            UpScale(scale=0,size=88, mode='linear'), #64*88 #Com scale=2 dava 72\n",
    "            \n",
    "\n",
    "            nn.ConvTranspose1d(64,32, kernel_size=(16,),stride=(1,),padding=(0,),output_padding=(0,),bias=True), #64*88 -> 32*103\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(32,16, kernel_size=(16,),stride=(1,),padding=(0,),output_padding=(0,),bias=True), #32*103 -> 16*118\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(16,8, kernel_size=(16,),stride=(2,),padding=(0,),output_padding=(0,),bias=True), #16*118 -> 8*250\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.ConvTranspose1d(8,1, kernel_size=(1,),stride=(1,),padding=(0,),output_padding=(0,),bias=True), #8*250 -> 1*250\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded= self.decoder(encoded)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lfunction1= nn.MSELoss()\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def calculator1(self, values, w):\n",
    "\n",
    "        la= torch.mul(torch.trapezoid(torch.div(values,w),w), 2)\n",
    "\n",
    "        #sim1=torch.mul(torch.div(values,w),torch.log(w))\n",
    "        #sim=torch.trapezoid(sim1,w)\n",
    "\n",
    "        #wlog2=torch.div(2,la)\n",
    "        #wlog1=torch.exp(torch.mul(wlog2,sim))\n",
    "        #wlog=torch.div(wlog1,K_BOLTZMANN_RY)\n",
    "\n",
    "        return la\n",
    "\n",
    "    def calculator2(self,la,wlog,mu):\n",
    "        \n",
    "        ad= torch.mul(torch.div(wlog,1.2),torch.exp(torch.div(torch.mul(torch.add(la,1),-1.04),torch.sub(la,torch.mul(torch.add(1,torch.mul(la,0.62)),mu)))))\n",
    "        ad= torch.where(la > 1.5*mu, ad,0)\n",
    "\n",
    "        return ad\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, inputs, targets, w):\n",
    "        inputs=torch.squeeze(inputs)\n",
    "        targets=torch.squeeze(targets)\n",
    "\n",
    "        w=torch.tensor(w.values/Ry2cm, dtype=torch.float32)\n",
    "        w=w.repeat(inputs.shape[0],1)\n",
    "\n",
    "        la1=self.calculator1(inputs,w)\n",
    "        la2=self.calculator1(targets,w)\n",
    "\n",
    "\n",
    "        return torch.mean(torch.pow(torch.sub(la1,la2),2)) +  Lfunction1(inputs,targets)*0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(250,8) \n",
    "\n",
    "Lfunction2= CustomLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50, factor=0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer =  SummaryWriter('runs/teste')\n",
    "\n",
    "checkpoint_path = \"current_checkpoint.pt\" \n",
    "best_model_path = \"best_model.pt\"\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    f_path = checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    loss_min = checkpoint['loss_min']\n",
    "    return model, optimizer, checkpoint['epoch'], loss_min.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 5.01968336, lr: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m     torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 24\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     loss_train\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     27\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(np\u001b[38;5;241m.\u001b[39mmean(loss_train))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outputs =[[],[]]\n",
    "epochs=20000\n",
    "epoch=0\n",
    "loss_train=[100000]\n",
    "lr_0=0\n",
    "loss_test=1000\n",
    "loss_test_list=[]\n",
    "train_loss_min=100000\n",
    "\n",
    "\n",
    "while epoch < epochs:\n",
    "    loss_train=[]\n",
    "    epoch+=1\n",
    "    model.train()\n",
    "    for x in train_data: \n",
    "        optimizer.zero_grad()\n",
    "        recon= model(x)\n",
    "        \n",
    "        loss=Lfunction2(x,recon,w)\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    " \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_train.append(loss.detach().numpy())\n",
    "\n",
    "    scheduler.step(np.mean(loss_train))\n",
    "    \n",
    "    \n",
    "    input_data = torch.tensor(test_set.values, dtype=torch.float32)[:,None]\n",
    "    predictions = model(input_data)\n",
    "    loss1=Lfunction2(input_data,predictions,w)\n",
    "    loss_test_list.append(loss1.detach().numpy())\n",
    "    \n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {np.mean(loss_train):.8f}, Test Loss: {loss1.detach().numpy():.8f}\")\n",
    "            \n",
    "    writer.add_scalar(\"Loss_scaled/train\", np.mean(loss_train), epoch)\n",
    "    writer.add_scalar(\"Loss_scaled_test/train\", loss1.detach().numpy(), epoch)\n",
    "    \n",
    "    if optimizer.param_groups[0]['lr'] != lr_0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {np.mean(loss_train):.8f}, lr: {optimizer.param_groups[0]['lr']}\")\n",
    "        lr_0=optimizer.param_groups[0]['lr']\n",
    "        \n",
    "\n",
    "    outputs[0].append(epoch)\n",
    "    outputs[1].append(np.mean(loss_train))\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'loss_min': np.mean(loss_train),\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()}\n",
    "    \n",
    "    if np.mean(loss_train) <= train_loss_min:\n",
    "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "        train_loss_min = np.mean(loss_train)\n",
    "        \n",
    "print(f\"Epoch: {epoch+1}, Loss: {np.mean(loss_train):.8f}, lr: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale('log')\n",
    "\n",
    "plt.plot(outputs[0],outputs[1], linewidth=1)\n",
    "plt.plot(outputs[0],loss_test_list, linewidth=1)\n",
    "plt.axhline(y=min(outputs[1]), color='r', linestyle='-',linewidth=1)\n",
    "print(min(outputs[1]))\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"loss_train_test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, start_epoch, loss_min = load_ckp(best_model_path , model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.tensor(test_set.values, dtype=torch.float32)[:,None]\n",
    "predictions = model(input_data)\n",
    "loss=Lfunction2(predictions,input_data,w)\n",
    "predictions=torch.squeeze(predictions)\n",
    "print(predictions.shape)\n",
    "predictions = pd.DataFrame(predictions.detach().numpy(),  columns=test_set.columns)\n",
    "print(loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_emb = model.encoder(input_data)\n",
    "latent_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(torch.squeeze(model.decoder(latent_emb.detach())).detach().numpy(), columns=test_set.columns, index=test_set.index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo dos Tc's e erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurate = []\n",
    "ae_values = []\n",
    "valores_pca=[]\n",
    "\n",
    "pca=PCA(n_components=8)\n",
    "data1=(pd.DataFrame(pca.inverse_transform(pca.fit_transform(test_set)), index=test_set.index))\n",
    "\n",
    "for i in test_set.index.tolist():\n",
    "  wlog, w2, la = get_epc(test_set.loc[i], w/Ry2cm)   \n",
    "  tc = get_Tc(la, wlog, 0.1)\n",
    "\n",
    "  wlog1, w21, la1 = get_epc(predictions.loc[i], w/Ry2cm)\n",
    "  tc1 = get_Tc(la1, wlog1, 0.1)\n",
    "\n",
    "  wlog2, w22, la2 = get_epc(data1.loc[i], w/Ry2cm)\n",
    "  tc2 = get_Tc(la2, wlog2, 0.1)\n",
    "\n",
    "  accurate.append(tc)\n",
    "  ae_values.append(tc1)\n",
    "  valores_pca.append(tc2)\n",
    "\n",
    "ae_values = pd.DataFrame(ae_values, index=test_set.index.tolist())\n",
    "accurate = pd.DataFrame(accurate, index=test_set.index.tolist())\n",
    "valores_pca = pd.DataFrame(valores_pca, index=test_set.index.tolist())\n",
    "\n",
    "errors_abs = (ae_values-accurate).abs()\n",
    "errors2_abs = (valores_pca-accurate).abs()\n",
    "\n",
    "print(len(accurate),len(ae_values),len(valores_pca))\n",
    "\n",
    "remover= list(accurate.loc[accurate[0] == 0].to_dict()[0].keys())\n",
    "\n",
    "accurate=accurate.drop(index=remover)\n",
    "ae_values=ae_values.drop(index=remover)\n",
    "valores_pca=valores_pca.drop(index=remover)\n",
    "\n",
    "\n",
    "print(len(accurate),len(ae_values),len(valores_pca))\n",
    "\n",
    "errors_rel =(ae_values-accurate).abs()/accurate\n",
    "errors2_rel =(valores_pca-accurate).abs()/accurate\n",
    "\n",
    "print(list(accurate.loc[accurate[0] == 0].to_dict()[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat, mat_a2f = random.choice(list(test_set.T.items()))\n",
    "\n",
    "print(mat)\n",
    "plt.plot(w, mat_a2f )\n",
    "print(len(w),len(mat_a2f))\n",
    "\n",
    "n=0\n",
    "for i in mat_a2f[::-1]:\n",
    "    if i == 0:\n",
    "        n+=1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "valores_nulos=[]\n",
    "\n",
    "for mat, mat_a2f in list(test_set.T.items()):\n",
    "    n2=0\n",
    "    for i in mat_a2f[::-1]:\n",
    "        if i == 0:\n",
    "            n2+=1\n",
    "        else:\n",
    "            valores_nulos.append(n2)\n",
    "            break\n",
    "print(min(valores_nulos),max(valores_nulos))\n",
    "print(valores_nulos)\n",
    "print(437-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.hist(errors_abs, bins=30, range=[0,10],histtype=\"step\",label=\"AE\")\n",
    "plt.hist(errors2_abs, label=\"PCA\", bins=30, range=[0,10],histtype=\"step\")\n",
    "\n",
    "print(np.average(errors_abs))\n",
    "print(np.average(errors2_abs))\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"hist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean2(lista):\n",
    "    return sum(lista)/len(lista)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.hist(errors_rel, bins=30, range=[0,10],histtype=\"step\",label=\"AE\")\n",
    "plt.hist(errors2_rel, label=\"PCA\", bins=30, range=[0,10],histtype=\"step\")\n",
    "\n",
    "print(len(errors_rel))\n",
    "print(len(errors2_rel))\n",
    "\n",
    "erros_grandes=list(errors_rel.loc[errors_rel[0] > 100].to_dict()[0].keys())\n",
    "erros2_grandes=list(errors2_rel.loc[errors2_rel[0] > 100].to_dict()[0].keys())\n",
    "\n",
    "print(len(erros_grandes))\n",
    "print(len(erros2_grandes))\n",
    "\n",
    "\n",
    "list1=errors_rel[0].to_list()\n",
    "list2=errors2_rel[0].to_list()\n",
    "\n",
    "print(mean2(list1))\n",
    "print(mean2(list2))\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"hist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica aleatoriamente os resultados vs PCA e AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AE\")\n",
    "mat, mat_a2f = random.choice(list(test_set.T.items()))\n",
    "for i in [mat]:\n",
    "  print(\"--------------------\")\n",
    "  plt.plot(w,test_set.loc[i],label=\"Original\")\n",
    "  plt.plot(w, predictions.loc[i],label=\"Recon AE\")\n",
    "  plt.plot(w, data1.loc[i],label=\"Recon PCA\")\n",
    "  plt.xlabel=\"w\"\n",
    "  plt.ylabel=\"a2f\"\n",
    "  plt.legend()\n",
    "  plt.show\n",
    "  print(i, accurate.loc[i][0],ae_values.loc[i][0],errors_rel.loc[i][0],errors_abs.loc[i][0])\n",
    "  print(i, accurate.loc[i][0],valores_pca.loc[i][0],errors2_rel.loc[i][0],errors2_abs.loc[i][0])\n",
    "  #plt.savefig(f\"pasta/figura_{i}_{errors_rel.loc[i][0]}.png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

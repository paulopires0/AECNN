{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import time \n",
    "\n",
    "from scipy.integrate import simpson\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"a2f_data.pk.bz2\")[:250] #Objetivo do [:250] é remover os pontos iguais a zero\n",
    "w = df.w\n",
    "df = df.drop(\"w\", axis=1).T\n",
    "df=df.drop([\"agm002693510\"]) #Está repetido com valores de NaN\n",
    "\n",
    "Ry2cm = 219474.63/2\n",
    "K_BOLTZMANN_RY = (1.380649E-23/(4.3597447222071E-18/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epc(a2f, w):\n",
    "  K_BOLTZMANN_RY = (1.380649E-23/(4.3597447222071E-18/2))\n",
    "  la = 2*simpson(a2f/w, w)\n",
    "  sim = simpson(a2f/w*np.log(w), w)\n",
    "  wlog = np.exp(2/la*sim)/K_BOLTZMANN_RY\n",
    "  sim = max(0.0, simpson(2*a2f*w, w))\n",
    "\n",
    "  w2 = np.sqrt(sim/la)/K_BOLTZMANN_RY\n",
    "  return wlog, w2, la\n",
    "\n",
    "def get_Tc(la, wlog, mu):\n",
    "  if (la > 1.5*mu) and (wlog > 0):\n",
    "    ad = wlog/1.2*np.exp(-(1.04*(1+la))/(la-mu*(1+0.62*la)))\n",
    "  else:\n",
    "    ad = 0.0\n",
    "  return ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.X_train = torch.tensor(data.values, dtype=torch.float32)[:,None]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_set, validation_set= np.split(df.sample(frac=1, random_state=52),[int(.1*len(df)),int(.9*len(df))])#[int(.8*len(df)),int(.9*len(df))])   random.randint(1, 100)[644,size_train+644]\n",
    "\n",
    "print(len(train_set),len(test_set),len(validation_set))\n",
    "print(len(train_set)/df.shape[0],len(test_set)/df.shape[0],len(validation_set)/df.shape[0])\n",
    "\n",
    "batch_size = train_set.shape[0]\n",
    "\n",
    "train_data= DataLoader(FeatureDataset(train_set), batch_size=128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpScale(nn.Module):\n",
    "    def __init__(self, size, scale, mode):\n",
    "        super(UpScale, self).__init__()\n",
    "        self.interpolate = nn.functional.interpolate\n",
    "        self.scale=scale\n",
    "        self.mode=mode\n",
    "        self.size=size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.size==0:\n",
    "            x = self.interpolate(x, scale_factor=self.scale, mode=self.mode, align_corners=False)\n",
    "            return x\n",
    "        elif self.scale==0:\n",
    "            x = self.interpolate(x, size=self.size, mode=self.mode, align_corners=False)\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, initial_size, min_size):\n",
    "        self.initial_size= initial_size\n",
    "        self.min_size= min_size\n",
    "\n",
    "        super(Autoencoder,self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(                                             #Channels*Dim\n",
    "            nn.Conv1d(1,8, kernel_size=(16,),stride=(2,),padding=(0,),bias=True), #1*250 -> 8*118\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.SiLU(), \n",
    "\n",
    "\n",
    "            nn.Conv1d(8, 16, kernel_size=(16,),stride=(1,),padding=(0,),bias=True), #8*118 -> 16*103\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(), \n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=(2,)), #16*103 -> 16*51\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=(16,),stride=(1,),padding=(0,),bias=True), #16*51 -> 32*36\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=(16,),stride=(2,),padding=(0,),bias=True), #32*36 -> 64*11\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Conv1d(64, 16, kernel_size=(8,),stride=(1,),padding=(0,),bias=True), #64*11 -> 16*4\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(64,32,bias=True),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(32,16,bias=True),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(16,8,bias=True),\n",
    "            nn.Tanh(), \n",
    "            )\n",
    "          \n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Linear(8,16,bias=True),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "\n",
    "            nn.Linear(16,32,bias=True),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(32,64,bias=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Unflatten(1,(16,4)),\n",
    "            \n",
    "            nn.ConvTranspose1d(16, 16, kernel_size=(8,),stride=(1,),padding=(0,),output_padding=(0,),bias=True), #16*4 -> 16*11\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.ConvTranspose1d(16, 64, kernel_size=(16,),stride=(2,),padding=(0,),output_padding=(0,),bias=True), #16*11 -> 64*36\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "\n",
    "\n",
    "            \n",
    "            UpScale(scale=0,size=88, mode='linear'), #64*88 #Com scale=2 dava 72\n",
    "            \n",
    "\n",
    "            nn.ConvTranspose1d(64,32, kernel_size=(16,),stride=(1,),padding=(0,),output_padding=(0,),bias=True), #64*88 -> 32*103\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(32,16, kernel_size=(16,),stride=(1,),padding=(0,),output_padding=(0,),bias=True), #32*103 -> 16*118\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(16,8, kernel_size=(16,),stride=(2,),padding=(0,),output_padding=(0,),bias=True), #16*118 -> 8*250\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.ConvTranspose1d(8,1, kernel_size=(1,),stride=(1,),padding=(0,),output_padding=(0,),bias=True), #8*250 -> 1*250\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded= self.decoder(encoded)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lfunction1= nn.MSELoss()\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def calculator1(self, values, w):\n",
    "\n",
    "        la= torch.mul(torch.trapezoid(torch.div(values,w),w), 2)\n",
    "\n",
    "        #sim1=torch.mul(torch.div(values,w),torch.log(w))\n",
    "        #sim=torch.trapezoid(sim1,w)\n",
    "\n",
    "        #wlog2=torch.div(2,la)\n",
    "        #wlog1=torch.exp(torch.mul(wlog2,sim))\n",
    "        #wlog=torch.div(wlog1,K_BOLTZMANN_RY)\n",
    "\n",
    "        return la\n",
    "\n",
    "    def calculator2(self,la,wlog,mu):\n",
    "        \n",
    "        ad= torch.mul(torch.div(wlog,1.2),torch.exp(torch.div(torch.mul(torch.add(la,1),-1.04),torch.sub(la,torch.mul(torch.add(1,torch.mul(la,0.62)),mu)))))\n",
    "        ad= torch.where(la > 1.5*mu, ad,0)\n",
    "\n",
    "        return ad\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, inputs, targets, w):\n",
    "        inputs=torch.squeeze(inputs)\n",
    "        targets=torch.squeeze(targets)\n",
    "\n",
    "        w=torch.tensor(w.values/Ry2cm, dtype=torch.float32)\n",
    "        w=w.repeat(inputs.shape[0],1)\n",
    "\n",
    "        la1=self.calculator1(inputs,w)\n",
    "        la2=self.calculator1(targets,w)\n",
    "\n",
    "\n",
    "        return torch.mean(torch.pow(torch.sub(la1,la2),2)) +  Lfunction1(inputs,targets)*0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(250,8) \n",
    "\n",
    "Lfunction2= CustomLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50, factor=0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer =  SummaryWriter('runs/teste')\n",
    "\n",
    "checkpoint_path = \"current_checkpoint.pt\" \n",
    "best_model_path = \"best_model.pt\"\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    f_path = checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    loss_min = checkpoint['loss_min']\n",
    "    return model, optimizer, checkpoint['epoch'], loss_min.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs =[[],[]]\n",
    "epochs=20000\n",
    "epoch=0\n",
    "loss_train=[100000]\n",
    "lr_0=0\n",
    "loss_test=1000\n",
    "loss_test_list=[]\n",
    "train_loss_min=100000\n",
    "\n",
    "\n",
    "while epoch < epochs:\n",
    "    loss_train=[]\n",
    "    epoch+=1\n",
    "    model.train()\n",
    "    for x in train_data: \n",
    "        optimizer.zero_grad()\n",
    "        recon= model(x)\n",
    "        \n",
    "        loss=Lfunction2(x,recon,w)\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    " \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_train.append(loss.detach().numpy())\n",
    "\n",
    "    scheduler.step(np.mean(loss_train))\n",
    "    \n",
    "    \n",
    "    input_data = torch.tensor(test_set.values, dtype=torch.float32)[:,None]\n",
    "    predictions = model(input_data)\n",
    "    loss1=Lfunction2(input_data,predictions,w)\n",
    "    loss_test_list.append(loss1.detach().numpy())\n",
    "    \n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {np.mean(loss_train):.8f}, Test Loss: {loss1.detach().numpy():.8f}\")\n",
    "            \n",
    "    writer.add_scalar(\"Loss_scaled/train\", np.mean(loss_train), epoch)\n",
    "    writer.add_scalar(\"Loss_scaled_test/train\", loss1.detach().numpy(), epoch)\n",
    "    \n",
    "    if optimizer.param_groups[0]['lr'] != lr_0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {np.mean(loss_train):.8f}, lr: {optimizer.param_groups[0]['lr']}\")\n",
    "        lr_0=optimizer.param_groups[0]['lr']\n",
    "        \n",
    "\n",
    "    outputs[0].append(epoch)\n",
    "    outputs[1].append(np.mean(loss_train))\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'loss_min': np.mean(loss_train),\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()}\n",
    "    \n",
    "    if np.mean(loss_train) <= train_loss_min:\n",
    "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "        train_loss_min = np.mean(loss_train)\n",
    "        \n",
    "print(f\"Epoch: {epoch+1}, Loss: {np.mean(loss_train):.8f}, lr: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale('log')\n",
    "\n",
    "plt.plot(outputs[0],outputs[1], linewidth=1)\n",
    "plt.plot(outputs[0],loss_test_list, linewidth=1)\n",
    "plt.axhline(y=min(outputs[1]), color='r', linestyle='-',linewidth=1)\n",
    "print(min(outputs[1]))\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"loss_train_test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, start_epoch, loss_min = load_ckp(best_model_path , model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.tensor(test_set.values, dtype=torch.float32)[:,None]\n",
    "predictions = model(input_data)\n",
    "loss=Lfunction2(predictions,input_data,w)\n",
    "predictions=torch.squeeze(predictions)\n",
    "print(predictions.shape)\n",
    "predictions = pd.DataFrame(predictions.detach().numpy(),  columns=test_set.columns)\n",
    "print(loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_emb = model.encoder(input_data)\n",
    "latent_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(torch.squeeze(model.decoder(latent_emb.detach())).detach().numpy(), columns=test_set.columns, index=test_set.index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo dos Tc's e erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurate = []\n",
    "ae_values = []\n",
    "valores_pca=[]\n",
    "\n",
    "pca=PCA(n_components=8)\n",
    "data1=(pd.DataFrame(pca.inverse_transform(pca.fit_transform(test_set)), index=test_set.index))\n",
    "\n",
    "for i in test_set.index.tolist():\n",
    "  wlog, w2, la = get_epc(test_set.loc[i], w/Ry2cm)   \n",
    "  tc = get_Tc(la, wlog, 0.1)\n",
    "\n",
    "  wlog1, w21, la1 = get_epc(predictions.loc[i], w/Ry2cm)\n",
    "  tc1 = get_Tc(la1, wlog1, 0.1)\n",
    "\n",
    "  wlog2, w22, la2 = get_epc(data1.loc[i], w/Ry2cm)\n",
    "  tc2 = get_Tc(la2, wlog2, 0.1)\n",
    "\n",
    "  accurate.append(tc)\n",
    "  ae_values.append(tc1)\n",
    "  valores_pca.append(tc2)\n",
    "\n",
    "ae_values = pd.DataFrame(ae_values, index=test_set.index.tolist())\n",
    "accurate = pd.DataFrame(accurate, index=test_set.index.tolist())\n",
    "valores_pca = pd.DataFrame(valores_pca, index=test_set.index.tolist())\n",
    "\n",
    "errors_abs = (ae_values-accurate).abs()\n",
    "errors2_abs = (valores_pca-accurate).abs()\n",
    "\n",
    "print(len(accurate),len(ae_values),len(valores_pca))\n",
    "\n",
    "remover= list(accurate.loc[accurate[0] == 0].to_dict()[0].keys())\n",
    "\n",
    "accurate=accurate.drop(index=remover)\n",
    "ae_values=ae_values.drop(index=remover)\n",
    "valores_pca=valores_pca.drop(index=remover)\n",
    "\n",
    "\n",
    "print(len(accurate),len(ae_values),len(valores_pca))\n",
    "\n",
    "errors_rel =(ae_values-accurate).abs()/accurate\n",
    "errors2_rel =(valores_pca-accurate).abs()/accurate\n",
    "\n",
    "print(list(accurate.loc[accurate[0] == 0].to_dict()[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat, mat_a2f = random.choice(list(test_set.T.items()))\n",
    "\n",
    "print(mat)\n",
    "plt.plot(w, mat_a2f )\n",
    "print(len(w),len(mat_a2f))\n",
    "\n",
    "n=0\n",
    "for i in mat_a2f[::-1]:\n",
    "    if i == 0:\n",
    "        n+=1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "valores_nulos=[]\n",
    "\n",
    "for mat, mat_a2f in list(test_set.T.items()):\n",
    "    n2=0\n",
    "    for i in mat_a2f[::-1]:\n",
    "        if i == 0:\n",
    "            n2+=1\n",
    "        else:\n",
    "            valores_nulos.append(n2)\n",
    "            break\n",
    "print(min(valores_nulos),max(valores_nulos))\n",
    "print(valores_nulos)\n",
    "print(437-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.hist(errors_abs, bins=30, range=[0,10],histtype=\"step\",label=\"AE\")\n",
    "plt.hist(errors2_abs, label=\"PCA\", bins=30, range=[0,10],histtype=\"step\")\n",
    "\n",
    "print(np.average(errors_abs))\n",
    "print(np.average(errors2_abs))\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"hist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean2(lista):\n",
    "    return sum(lista)/len(lista)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.hist(errors_rel, bins=30, range=[0,10],histtype=\"step\",label=\"AE\")\n",
    "plt.hist(errors2_rel, label=\"PCA\", bins=30, range=[0,10],histtype=\"step\")\n",
    "\n",
    "print(len(errors_rel))\n",
    "print(len(errors2_rel))\n",
    "\n",
    "erros_grandes=list(errors_rel.loc[errors_rel[0] > 100].to_dict()[0].keys())\n",
    "erros2_grandes=list(errors2_rel.loc[errors2_rel[0] > 100].to_dict()[0].keys())\n",
    "\n",
    "print(len(erros_grandes))\n",
    "print(len(erros2_grandes))\n",
    "\n",
    "\n",
    "list1=errors_rel[0].to_list()\n",
    "list2=errors2_rel[0].to_list()\n",
    "\n",
    "print(mean2(list1))\n",
    "print(mean2(list2))\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"hist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica aleatoriamente os resultados vs PCA e AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AE\")\n",
    "mat, mat_a2f = random.choice(list(test_set.T.items()))\n",
    "for i in [mat]:\n",
    "  print(\"--------------------\")\n",
    "  plt.plot(w,test_set.loc[i],label=\"Original\")\n",
    "  plt.plot(w, predictions.loc[i],label=\"Recon AE\")\n",
    "  plt.plot(w, data1.loc[i],label=\"Recon PCA\")\n",
    "  plt.xlabel=\"w\"\n",
    "  plt.ylabel=\"a2f\"\n",
    "  plt.legend()\n",
    "  plt.show\n",
    "  print(i, accurate.loc[i][0],ae_values.loc[i][0],errors_rel.loc[i][0],errors_abs.loc[i][0])\n",
    "  print(i, accurate.loc[i][0],valores_pca.loc[i][0],errors2_rel.loc[i][0],errors2_abs.loc[i][0])\n",
    "  #plt.savefig(f\"pasta/figura_{i}_{errors_rel.loc[i][0]}.png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
